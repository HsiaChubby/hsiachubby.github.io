<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="kubernetes,">










<meta name="description" content="[TOC] 一、环境准备1.1 硬件设备环境采用5台腾讯云的CVM作为kubernetes的部署环境，具体信息如下：    主机名 IP 配置 备注     （Old）VM_0_17_centos；（New）tf-k8s-m1 172.21.0.17 4c 8g k8s的master，同时也是etcd节点   （Old）VM_0_42_centos；（New）tf-k8s-m2 172.21.0.">
<meta name="keywords" content="kubernetes">
<meta property="og:type" content="article">
<meta property="og:title" content="kubeadm1.13创建HAkubernetes集群">
<meta property="og:url" content="http://yoursite.com/2019/02/18/kubeadm1-13创建HAkubernetes集群/index.html">
<meta property="og:site_name" content="Chubby的博客">
<meta property="og:description" content="[TOC] 一、环境准备1.1 硬件设备环境采用5台腾讯云的CVM作为kubernetes的部署环境，具体信息如下：    主机名 IP 配置 备注     （Old）VM_0_17_centos；（New）tf-k8s-m1 172.21.0.17 4c 8g k8s的master，同时也是etcd节点   （Old）VM_0_42_centos；（New）tf-k8s-m2 172.21.0.">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-02-18T08:56:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="kubeadm1.13创建HAkubernetes集群">
<meta name="twitter:description" content="[TOC] 一、环境准备1.1 硬件设备环境采用5台腾讯云的CVM作为kubernetes的部署环境，具体信息如下：    主机名 IP 配置 备注     （Old）VM_0_17_centos；（New）tf-k8s-m1 172.21.0.17 4c 8g k8s的master，同时也是etcd节点   （Old）VM_0_42_centos；（New）tf-k8s-m2 172.21.0.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/02/18/kubeadm1-13创建HAkubernetes集群/">





  <title>kubeadm1.13创建HAkubernetes集群 | Chubby的博客</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chubby的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/18/kubeadm1-13创建HAkubernetes集群/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chubby Hsia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chubby的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">kubeadm1.13创建HAkubernetes集群</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-18T10:13:14+08:00">
                2019-02-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><h3 id="1-1-硬件设备环境"><a href="#1-1-硬件设备环境" class="headerlink" title="1.1 硬件设备环境"></a>1.1 硬件设备环境</h3><p>采用5台腾讯云的CVM作为kubernetes的部署环境，具体信息如下：</p>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP</th>
<th>配置</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>（Old）VM_0_17_centos；（New）tf-k8s-m1</td>
<td>172.21.0.17</td>
<td>4c 8g</td>
<td>k8s的master，同时也是etcd节点</td>
</tr>
<tr>
<td>（Old）VM_0_42_centos；（New）tf-k8s-m2</td>
<td>172.21.0.42</td>
<td>4c 8g</td>
<td>k8s的master，同时也是etcd节点</td>
</tr>
<tr>
<td>（Old）VM_0_18_centos；（New）tf-k8s-m3</td>
<td>172.21.0.18</td>
<td>4c 8g</td>
<td>k8s的master，同时也是etcd节点</td>
</tr>
<tr>
<td>（Old）VM_0_20_centos；（New）tf-k8s-n1</td>
<td>172.21.0.20</td>
<td>4c 8g</td>
<td>工作节点 node，容器编排最终 pod 工作节点</td>
</tr>
<tr>
<td>（Old）VM_0_47_centos；（New）tf-k8s-n2</td>
<td>172.21.0.47</td>
<td>4c 8g</td>
<td>工作节点 node，容器编排最终 pod 工作节点</td>
</tr>
</tbody>
</table>
<h3 id="1-2-软件环境"><a href="#1-2-软件环境" class="headerlink" title="1.2 软件环境"></a>1.2 软件环境</h3><table>
<thead>
<tr>
<th>环境</th>
<th>简介</th>
</tr>
</thead>
<tbody>
<tr>
<td>操作系统</td>
<td>CentOS7</td>
</tr>
<tr>
<td>kubeadm</td>
<td>1.13.3</td>
</tr>
<tr>
<td>kubernetes</td>
<td>1.13.3</td>
</tr>
<tr>
<td>Docker</td>
<td>docker-ce 18.06.2</td>
</tr>
</tbody>
</table>
<h3 id="1-3-相关系统设置"><a href="#1-3-相关系统设置" class="headerlink" title="1.3 相关系统设置"></a>1.3 相关系统设置</h3><p>在正式安装之前，需要在每台机器上对以下配置进行修改：</p>
<ul>
<li>关闭防火墙，selinux</li>
<li>关闭系统的swap功能</li>
<li>关闭Linux swap空间的swappiness</li>
<li>配置L2网桥在转发包时会被iptables的FORWARD规则所过滤，该配置被CNI插件需要，更多信息请参考<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements" target="_blank" rel="noopener">Network Plugin Requirements</a></li>
<li>升级内核到最新（centos7 默认的内核是3.10.0-862.el7.x86_64 ,可以使用命令‘uname -a’进行查看），原因见<a href="https://github.com/Lentil1016/kubeadm-ha/issues/19" target="_blank" rel="noopener">请问下为什么要用4.18版本内核</a></li>
<li>开启IPVS</li>
<li>修改主机名（如果主机名中含有一些特殊字符，则需要调整主机名，不然在后续操作中会出现错误）<br>具体的配置修改执行脚本如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"># ---------- 关闭防火墙和selinux -----------</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config</span><br><span class="line"></span><br><span class="line"># ---------- 关闭交换分区 -----------</span><br><span class="line">swapoff -a</span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"># ---------- 设置网桥包经IPTables，core文件生成路径 -----------</span><br><span class="line">echo &quot;&quot;&quot;</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">&quot;&quot;&quot; &gt; /etc/sysctl.conf</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">sysctl -p</span><br><span class="line"></span><br><span class="line"># ---------- 同步时间 -----------</span><br><span class="line">yum install -y ntpdate</span><br><span class="line">ntpdate -u ntp.api.bz</span><br><span class="line"></span><br><span class="line"># ---------- 升级内核 -----------</span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y</span><br><span class="line"># 查看启动配置里是否有最新的内核</span><br><span class="line">cat /boot/grub2/grub.cfg | grep menuentry</span><br><span class="line"># 修改默认启动项</span><br><span class="line">grub2-set-default 0</span><br><span class="line"># 检查默认内核版本是否大于4.14，否则请调整默认启动参数</span><br><span class="line">grub2-editenv list</span><br><span class="line">#重启以更换内核</span><br><span class="line">reboot</span><br><span class="line">#查看内核信息</span><br><span class="line">uname -a</span><br><span class="line"></span><br><span class="line"># ---------- 确认内核版本后，开启IPVS -----------</span><br><span class="line">uname -a</span><br><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line">#!/bin/bash</span><br><span class="line">ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack&quot;</span><br><span class="line">for kernel_module in \$&#123;ipvs_modules&#125;; do</span><br><span class="line"> /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1</span><br><span class="line"> if [ $? -eq 0 ]; then</span><br><span class="line"> /sbin/modprobe \$&#123;kernel_module&#125;</span><br><span class="line"> fi</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</span><br><span class="line"></span><br><span class="line"># ---------- 修改主机名 -----------</span><br><span class="line"># 这里以VM_0_17_centos主机为例，其他的主机分别修改成相应的主机名</span><br><span class="line">hostnamectl set-hostname tf-k8s-m1</span><br></pre></td></tr></table></figure>
<h3 id="1-3-配置集群内各个机器之间的免密码登录"><a href="#1-3-配置集群内各个机器之间的免密码登录" class="headerlink" title="1.3 配置集群内各个机器之间的免密码登录"></a>1.3 配置集群内各个机器之间的免密码登录</h3><h4 id="1-3-1-配置hosts"><a href="#1-3-1-配置hosts" class="headerlink" title="1.3.1 配置hosts"></a>1.3.1 配置hosts</h4><p>为了便于后续的操作，我们需要给每一台设备配置下hosts域名信息，具体如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># vi /etc/hosts</span><br><span class="line">172.21.0.17 tf-k8s-m1 api.tf-k8s.xiangwushuo.com</span><br><span class="line">172.21.0.42 tf-k8s-m2</span><br><span class="line">172.21.0.18 tf-k8s-m3</span><br><span class="line">172.21.0.20 tf-k8s-n1</span><br><span class="line">172.21.0.47 tf-k8s-n2</span><br><span class="line">172.21.0.17 dashboard.tf-k8s.xiangwushuo.com</span><br></pre></td></tr></table></figure>
<h4 id="1-3-2-新建用户"><a href="#1-3-2-新建用户" class="headerlink" title="1.3.2 新建用户"></a>1.3.2 新建用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># useradd kube</span><br><span class="line"># visudo</span><br><span class="line">%wheel  ALL=(ALL)       ALL</span><br><span class="line">kube    ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure>
<p>备注：visudo命令是用来给kube用户添加sudo密码</p>
<h4 id="1-3-3-设置免密登录"><a href="#1-3-3-设置免密登录" class="headerlink" title="1.3.3 设置免密登录"></a>1.3.3 设置免密登录</h4><ol>
<li>各个设备的root用户&amp;kube用户（不同用户配置不同的）都生成各自的免密登录的ssh的私钥与公钥</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## 为root用户生成ssh的私钥与公钥</span><br><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure>
<p>在/root目录下，会生成一个.ssh目录，.ssh目录下会生成以下三个文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-rw------- 1 root root 2398 Feb 13 15:18 authorized_keys</span><br><span class="line">-rw------- 1 root root 1679 Feb 13 14:47 id_rsa</span><br><span class="line">-rw-r--r-- 1 root root  401 Feb 13 14:47 id_rsa.pub</span><br></pre></td></tr></table></figure>
<p>authorized_keys文件存储了本设备认证授权的其他设备的公钥信息；id_rsa存储了本设备的私钥信息；id_rsa.pub存储了本设备的公钥信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">## 为kube用户生成ssh的私钥与公钥</span><br><span class="line">su kube</span><br><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure>
<p>在/home/kube目录下，会生成一个.ssh目录，并包含相关文件。</p>
<ol start="2">
<li>各个设备上都创建好各自的ssh免密登录公钥与私钥后，需要将各自的公钥copy至其他的设备上，并将公钥信息添加到各个设备的authorized_keys文件中。<br>备注：也需要将各个节点自己的公钥copy至自己的authorized_keys文件中，这样自己才可以ssh自己。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">## 将每一台节点上的公钥都同步到相应的目录下</span><br><span class="line"># ll</span><br><span class="line">-rw-r--r-- 1 root root  401 Feb 13 15:15 tf-k8s-m1-id_rsa.pub</span><br><span class="line">-rw-r--r-- 1 root root  401 Feb 13 15:15 tf-k8s-m2-id_rsa.pub</span><br><span class="line">-rw-r--r-- 1 root root  401 Feb 13 15:15 tf-k8s-m3-id_rsa.pub</span><br><span class="line">-rw-r--r-- 1 root root  401 Feb 13 15:15 tf-k8s-n1-id_rsa.pub</span><br><span class="line">-rw-r--r-- 1 root root  401 Feb 13 15:15 tf-k8s-n2-id_rsa.pub</span><br><span class="line">## 将每台节点的公钥追加至authorized_keys文件中</span><br><span class="line">cat tf-k8s-m1-id_rsa.pub &gt; authorized_keys</span><br><span class="line">cat tf-k8s-m2-id_rsa.pub &gt; authorized_keys</span><br><span class="line">cat tf-k8s-m3-id_rsa.pub &gt; authorized_keys</span><br><span class="line">cat tf-k8s-n1-id_rsa.pub &gt; authorized_keys</span><br><span class="line">cat tf-k8s-n2-id_rsa.pub &gt; authorized_keys</span><br></pre></td></tr></table></figure>
<p>测试是否能够正常使用ssh免密登录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh root@tf-k8s-m1</span><br><span class="line">ssh root@tf-k8s-m2</span><br><span class="line">ssh root@tf-k8s-m3</span><br><span class="line">ssh root@tf-k8s-n1</span><br><span class="line">ssh root@tf-k8s-n2</span><br></pre></td></tr></table></figure>
<p><strong>提示</strong>：如果其他机器上的 root 下的 /root/.ssh/authorized_keys 不存在，可以手动创建。要注意的是：authorized_keys 的权限需要是 600。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## 如果 authorized_keys 的权限不是 600，执行修改权限的命令。</span><br><span class="line">chmod 600 authorized_keys</span><br></pre></td></tr></table></figure>
<h2 id="二、安装步骤"><a href="#二、安装步骤" class="headerlink" title="二、安装步骤"></a>二、安装步骤</h2><p>以下操作，可以都切换至kube用户下进行操作。</p>
<h3 id="2-1-安装docker"><a href="#2-1-安装docker" class="headerlink" title="2.1 安装docker"></a>2.1 安装docker</h3><p>由于kubeadm的ha模式对docker的版本是有一定的要求的，因此，本教程中安装官方推荐的docker版本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 安装依赖包</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line"></span><br><span class="line"># 添加Docker软件包源</span><br><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line">#关闭测试版本list（只显示稳定版）</span><br><span class="line">sudo yum-config-manager --enable docker-ce-edge</span><br><span class="line">sudo yum-config-manager --enable docker-ce-test</span><br><span class="line"></span><br><span class="line"># 更新yum包索引</span><br><span class="line">yum makecache fast</span><br><span class="line"></span><br><span class="line">#NO.1 指定版本安装</span><br><span class="line">yum list docker-ce --showduplicates|sort -r  </span><br><span class="line">yum install docker-ce-18.06.2.ce -y</span><br></pre></td></tr></table></figure>
<p>为了方便操作，我们在tf-k8s-m1节点上，创建一个批量部署docker的脚本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">## 创建install.docker.sh</span><br><span class="line"></span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">vhosts=&quot;tf-k8s-m1 tf-k8s-m2 tf-k8s-m3 tf-k8s-n1 tf-k8s-n2&quot;</span><br><span class="line"></span><br><span class="line">for h in $vhosts</span><br><span class="line">do</span><br><span class="line">    echo &quot;Install docker for $h&quot;</span><br><span class="line">    ssh kube@$h &quot;sudo yum install docker-ce-18.06.2.ce -y &amp;&amp; sudo systemctl enable docker &amp;&amp; systemctl start docker&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>执行install.docker.sh脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x install.docker.sh</span><br><span class="line">sh ./install.docker.sh</span><br></pre></td></tr></table></figure>
<h3 id="2-2-安装kubernetes-yum源和kubelet、kubeadm、kubectl组件"><a href="#2-2-安装kubernetes-yum源和kubelet、kubeadm、kubectl组件" class="headerlink" title="2.2 安装kubernetes yum源和kubelet、kubeadm、kubectl组件"></a>2.2 安装kubernetes yum源和kubelet、kubeadm、kubectl组件</h3><h4 id="2-2-1-所有机器上配置-kubernetes-repo-yum-源"><a href="#2-2-1-所有机器上配置-kubernetes-repo-yum-源" class="headerlink" title="2.2.1 所有机器上配置 kubernetes.repo yum 源"></a>2.2.1 所有机器上配置 kubernetes.repo yum 源</h4><p>详细的安装脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">## 创建脚本：install.k8s.repo.sh</span><br><span class="line"></span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">vhost=&quot;tf-k8s-m1 tf-k8s-m2 tf-k8s-m3 tf-k8s-n1 tf-k8s-n2&quot;</span><br><span class="line"></span><br><span class="line">## 设置为阿里云 kubernetes 仓库</span><br><span class="line">cat &lt;&lt;EOF &gt; kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mvCmd=&quot;sudo cp ~/kubernetes.repo /etc/yum.repos.d/&quot;</span><br><span class="line">for h in $vhost</span><br><span class="line">do</span><br><span class="line">  echo &quot;Setup kubernetes repository for $h&quot;</span><br><span class="line">  scp ./kubernetes.repo kube@$h:~</span><br><span class="line">  ssh kube@$h $mvCmd</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>执行install.k8s.repo.sh脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x install.k8s.repo.sh</span><br><span class="line">sh ./install.k8s.repo.sh</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2-所有机器上安装-kubelet、kubeadm、kubectl组件"><a href="#2-2-2-所有机器上安装-kubelet、kubeadm、kubectl组件" class="headerlink" title="2.2.2 所有机器上安装 kubelet、kubeadm、kubectl组件"></a>2.2.2 所有机器上安装 kubelet、kubeadm、kubectl组件</h4><p>详细安装脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">## 创建脚本：install.k8s.basic.sh</span><br><span class="line"></span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">vhost=&quot;tf-k8s-m1 tf-k8s-m2 tf-k8s-m3 tf-k8s-n1 tf-k8s-n2&quot;</span><br><span class="line"></span><br><span class="line">## 安装 kubelet kubeadm kubectl</span><br><span class="line">installCmd=&quot;sudo yum install -y kubelet kubeadm kubectl &amp;&amp; sudo systemctl enable kubelet&quot;</span><br><span class="line">for h in $vhost</span><br><span class="line">do</span><br><span class="line">  echo &quot;Install kubelet kubeadm kubectl for : $h&quot;</span><br><span class="line">  ssh kube@$h $installCmd</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>执行install.k8s.baisc.sh脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x install.k8s.basic.sh</span><br><span class="line">sh ./install.k8s.basic.sh</span><br></pre></td></tr></table></figure>
<h3 id="2-3-初始化kubeadm配置文件"><a href="#2-3-初始化kubeadm配置文件" class="headerlink" title="2.3 初始化kubeadm配置文件"></a>2.3 初始化kubeadm配置文件</h3><p>创建三台master机器tf-k8s-m1,tf-k8s-m2,tf-k8s-m3的kubeadm配置文件，其中主要是配置生成证书的域配置、etcd集群配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">## 创建脚本：init.kubeadm.config.sh</span><br><span class="line"></span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">## 1. 配置参数 </span><br><span class="line">## vhost 主机名和 vhostIP IP 一一对应</span><br><span class="line">vhost=(tf-k8s-m1 tf-k8s-m2 tf-k8s-m3)</span><br><span class="line">vhostIP=(172.21.0.17 172.21.0.42 172.21.0.18)</span><br><span class="line"></span><br><span class="line">domain=api.tf-k8s.xiangwushuo.com</span><br><span class="line"></span><br><span class="line">## etcd 初始化 m01 m02 m03 集群配置</span><br><span class="line">etcdInitCluster=(</span><br><span class="line">tf-k8s-m1=https://172.21.0.17:2380</span><br><span class="line">tf-k8s-m1=https://172.21.0.17:2380,tf-k8s-m2=https://172.21.0.42:2380</span><br><span class="line">tf-k8s-m1=https://172.21.0.17:2380,tf-k8s-m2=https://172.21.0.42:2380,tf-k8s-m3=https://172.21.0.18:2380</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">## etcd 初始化时，m01 m02 m03 分别的初始化集群状态</span><br><span class="line">initClusterStatus=(</span><br><span class="line">new</span><br><span class="line">existing</span><br><span class="line">existing</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 2.遍历 master 主机名和对应 IP</span><br><span class="line">## 生成对应的 kubeadmn 配置文件 </span><br><span class="line">for i in `seq 0 $(($&#123;#vhost[*]&#125;-1))`</span><br><span class="line">do</span><br><span class="line"></span><br><span class="line">h=$&#123;vhost[$&#123;i&#125;]&#125; </span><br><span class="line">ip=$&#123;vhostIP[$&#123;i&#125;]&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;--&gt; $h - $ip&quot;</span><br><span class="line">  </span><br><span class="line">## 生成 kubeadm 配置模板</span><br><span class="line">cat &lt;&lt;EOF &gt; kubeadm-config.$h.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: $ip</span><br><span class="line">  bindPort: 6443</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.13.3</span><br><span class="line"></span><br><span class="line"># 指定阿里云镜像仓库</span><br><span class="line">imageRepository: registry.aliyuncs.com/google_containers</span><br><span class="line"></span><br><span class="line"># apiServerCertSANs 填所有的 masterip、lbip、其它可能需要通过它访问 apiserver 的地址、域名或主机名等，</span><br><span class="line"># 如阿里fip，证书中会允许这些ip</span><br><span class="line"># 这里填一个自定义的域名</span><br><span class="line">apiServer:</span><br><span class="line">  certSANs:</span><br><span class="line">  - &quot;$domain&quot;</span><br><span class="line">controlPlaneEndpoint: &quot;$domain:6443&quot;</span><br><span class="line"></span><br><span class="line">## Etcd 配置</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    extraArgs:</span><br><span class="line">      listen-client-urls: &quot;https://127.0.0.1:2379,https://$ip:2379&quot;</span><br><span class="line">      advertise-client-urls: &quot;https://$ip:2379&quot;</span><br><span class="line">      listen-peer-urls: &quot;https://$ip:2380&quot;</span><br><span class="line">      initial-advertise-peer-urls: &quot;https://$ip:2380&quot;</span><br><span class="line">      initial-cluster: &quot;$&#123;etcdInitCluster[$&#123;i&#125;]&#125;&quot;</span><br><span class="line">      initial-cluster-state: $&#123;initClusterStatus[$&#123;i&#125;]&#125;</span><br><span class="line">    serverCertSANs:</span><br><span class="line">      - $h</span><br><span class="line">      - $ip</span><br><span class="line">    peerCertSANs:</span><br><span class="line">      - $h</span><br><span class="line">      - $ip</span><br><span class="line">networking:</span><br><span class="line">  podSubnet: &quot;10.244.0.0/16&quot;</span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">echo &quot;kubeadm-config.$h.yaml created ... ok&quot;</span><br><span class="line"></span><br><span class="line">## 3. 分发到其他 master 机器 </span><br><span class="line">scp kubeadm-config.$h.yaml kube@$h:~</span><br><span class="line">echo &quot;scp kubeadm-config.$h.yaml ... ok&quot;</span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>执行init.kubeadm.config.sh脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x init.kubeadm.config.sh</span><br><span class="line">sh ./init.kubeadm.config.sh</span><br></pre></td></tr></table></figure>
<p>执行成功之后，可以在tf-k8s-m1, tf-k8s-m2, tf-k8s-m3的 kube 用户的 home 目录（/home/kube）能看到对应的 kubeadm-config.tf-k8s-m1*.yaml 配置文件。 这个配置文件主要是用于后续初始化集群其他 master 的证书、 etcd 配置、kubelet 配置、kube-apiserver配置、kube-controller-manager 配置等。<br>各个master节点上对应的kubeadm配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cvm tf-k8s-m1：kubeadm-config.tf-k8s-m1.yaml</span><br><span class="line">cvm tf-k8s-m2：kubeadm-config.tf-k8s-m2.yaml</span><br><span class="line">cvm tf-k8s-m3：kubeadm-config.tf-k8s-m3.yaml</span><br></pre></td></tr></table></figure>
<h3 id="2-4-安装master镜像和执行kubeadm初始化"><a href="#2-4-安装master镜像和执行kubeadm初始化" class="headerlink" title="2.4 安装master镜像和执行kubeadm初始化"></a>2.4 安装master镜像和执行kubeadm初始化</h3><h4 id="2-4-1-拉取镜像到本地"><a href="#2-4-1-拉取镜像到本地" class="headerlink" title="2.4.1 拉取镜像到本地"></a>2.4.1 拉取镜像到本地</h4><p>因为 k8s.gcr.io 国内无法访问，我们可以选择通过阿里云的镜像仓库（kubeadm-config.tf-k8s-m1*.yaml 配置文件中已经指定使用阿里云镜像仓库  registry.aliyuncs.com/google_containers），将所需的镜像 pull 到本地。<br>我们可以通过以下命令，来查看是否已经成功指定了阿里云的镜像仓库,在tf-k8s-m1机器上，通过<code>kubeadm config images list</code>命令来查看，结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m1 ~]$ kubeadm config images list --config kubeadm-config.tf-k8s-m1.yaml</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-apiserver:v1.13.3</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-controller-manager:v1.13.3</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-scheduler:v1.13.3</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-proxy:v1.13.3</span><br><span class="line">registry.aliyuncs.com/google_containers/pause:3.1</span><br><span class="line">registry.aliyuncs.com/google_containers/etcd:3.2.24</span><br><span class="line">registry.aliyuncs.com/google_containers/coredns:1.2.6</span><br></pre></td></tr></table></figure>
<p>接下来，分别在tf-k8s-m1、tf-k8s-m2、tf-k8s-m3机器上，拉取相关镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m1 ~]$ sudo kubeadm config images pull --config kubeadm-config.tf-k8s-m1.yaml</span><br><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm config images pull --config kubeadm-config.tf-k8s-m2.yaml</span><br><span class="line">[kube@tf-k8s-m3 ~]$ sudo kubeadm config images pull --config kubeadm-config.tf-k8s-m3.yaml</span><br></pre></td></tr></table></figure>
<p>执行成功后，应该能够看到本地已经拉取的镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m1 ~]$ sudo docker images</span><br><span class="line">REPOSITORY                                                                     TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-apiserver                         v1.13.3             fe242e556a99        2 weeks ago         181MB</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-proxy                             v1.13.3             98db19758ad4        2 weeks ago         80.3MB</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-controller-manager                v1.13.3             0482f6400933        2 weeks ago         146MB</span><br><span class="line">registry.aliyuncs.com/google_containers/kube-scheduler                         v1.13.3             3a6f709e97a0        2 weeks ago         79.6MB</span><br><span class="line">quay.io/coreos/flannel                                                         v0.11.0-amd64       ff281650a721        2 weeks ago         52.6MB</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller   0.21.0              01bd760f276c        2 months ago        568MB</span><br><span class="line">registry.aliyuncs.com/google_containers/coredns                                1.2.6               f59dcacceff4        3 months ago        40MB</span><br><span class="line">registry.aliyuncs.com/google_containers/etcd                                   3.2.24              3cab8e1b9802        5 months ago        220MB</span><br><span class="line">registry.aliyuncs.com/google_containers/pause                                  3.1                 da86e6ba6ca1        14 months ago       742kB</span><br></pre></td></tr></table></figure>
<h4 id="2-4-2-安装master-tf-k8s-m1"><a href="#2-4-2-安装master-tf-k8s-m1" class="headerlink" title="2.4.2 安装master tf-k8s-m1"></a>2.4.2 安装master tf-k8s-m1</h4><p>我们目标是要搭建一个高可用的 master 集群，所以需要在三台 master tf-k8s-m1 tf-k8s-m2 tf-k8s-m3机器上分别通过 kubeadm 进行初始化。<br>由于 tf-k8s-m2 和 tf-k8s-m3 的初始化需要依赖 tf-k8s-m1 初始化成功后所生成的证书文件，所以这里需要先在 m01 初始化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m1 ~]$  sudo kubeadm init --config kubeadm-config.tf-k8s-m1.yaml</span><br></pre></td></tr></table></figure>
<p>初始化成功后，会看到如下日志：<br><strong>备注：如果初始化失败，则可以通过<code>kubeadm reset --force</code>命令重置之前kubeadm init命令的执行结果，恢复一个干净的环境</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">[init] Using Kubernetes version: v1.13.3</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Activating the kubelet service</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [m01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local api.k8s.hiko.im api.k8s.hiko.im] and IPs [10.96.0.1 10.0.2.15]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [m01 localhost m01] and IPs [10.0.2.15 127.0.0.1 ::1 192.168.33.10]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [m01 localhost m01] and IPs [10.0.2.15 127.0.0.1 ::1 192.168.33.10]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 19.009523 seconds</span><br><span class="line">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.13&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;m01&quot; as an annotation</span><br><span class="line">[mark-control-plane] Marking the node m01 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class="line">[mark-control-plane] Marking the node m01 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: a1t7c1.mzltpc72dc3wzj9y</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join api.k8s.hiko.im:6443 --token a1t7c1.mzltpc72dc3wzj9y --discovery-token-ca-cert-hash sha256:05f44b111174613055975f012fc11fe09bdcd746bd7b3c8d99060c52619f8738</span><br></pre></td></tr></table></figure>
<p>至此，就完成了第一台master的初始化工作。</p>
<h4 id="2-4-3-kube用户配置"><a href="#2-4-3-kube用户配置" class="headerlink" title="2.4.3 kube用户配置"></a>2.4.3 kube用户配置</h4><p>为了让tf-k8s-m1的 kube 用户能通过 kubectl 管理集群，接着我们需要给tf-k8s-m1 的 kube 用户配置管理集群的配置。在tf-k8s-m1机器上创建config.using.cluster.sh脚本，具体如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## 创建脚本：config.using.cluster.sh</span><br><span class="line"></span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line"># 为 kube 用户配置</span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<p>执行config.using.cluster.sh脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x config.using.cluster.sh</span><br><span class="line">sh ./config.using.cluster.sh</span><br></pre></td></tr></table></figure>
<p>验证结果，通过<code>kubectl</code>命令查看集群状态，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m1 ~]$ kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https://api.tf-k8s.xiangwushuo.com:6443</span><br><span class="line">KubeDNS is running at https://api.tf-k8s.xiangwushuo.com:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &apos;kubectl cluster-info dump&apos;.</span><br></pre></td></tr></table></figure>
<p>查看集群所有的pods信息，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m1 ~]$ kubectl get pods --all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   coredns-78d4cf999f-cw79l      0/1     Pending   0          47m</span><br><span class="line">kube-system   coredns-78d4cf999f-w8j47      0/1     Pending   0          47m</span><br><span class="line">kube-system   etcd-m01                      1/1     Running   0          47m</span><br><span class="line">kube-system   kube-apiserver-m01            1/1     Running   0          46m</span><br><span class="line">kube-system   kube-controller-manager-m01   1/1     Running   0          46m</span><br><span class="line">kube-system   kube-proxy-5954k              1/1     Running   0          47m</span><br><span class="line">kube-system   kube-scheduler-m01            1/1     Running   0          47m</span><br></pre></td></tr></table></figure>
<p>其中，由于未安装相关的网络组件，eg:flannel,所有coredn还是显示为pending，暂时没有影响。</p>
<h4 id="2-4-4-安装CNI插件flannel"><a href="#2-4-4-安装CNI插件flannel" class="headerlink" title="2.4.4 安装CNI插件flannel"></a>2.4.4 安装CNI插件flannel</h4><p><strong>备注：所有的节点都需要安装</strong><br>具体的安装脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## 拉取镜像</span><br><span class="line">sudo docker pull quay.io/coreos/flannel:v0.11.0-amd64</span><br><span class="line">## 部署</span><br><span class="line">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure>
<p>安装成功之后，通过 <code>kubectl get pods --all-namespaces</code>，看到所有 Pod 都正常了.</p>
<h3 id="2-5-安装剩余的master"><a href="#2-5-安装剩余的master" class="headerlink" title="2.5 安装剩余的master"></a>2.5 安装剩余的master</h3><h4 id="2-5-1-同步tf-k8s-m1的ca证书"><a href="#2-5-1-同步tf-k8s-m1的ca证书" class="headerlink" title="2.5.1 同步tf-k8s-m1的ca证书"></a>2.5.1 同步tf-k8s-m1的ca证书</h4><p>首先，将 tf-k8s-m1 中的 ca 证书，scp 到其他 master 机器（tf-k8s-m2 tf-k8s-m3）。<br>为了方便，这里也是通过脚本来执行，具体如下：<br>注意：需要确认 tf-k8s-m1 上的 root 账号可以免密登录到 tf-k8s-m2 和 tf-k8s-m3 的 root 账号。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">## 创建脚本：sync.master.ca.sh</span><br><span class="line"></span><br><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">vhost=&quot;tf-k8s-m2 tf-k8s-m3&quot;</span><br><span class="line">usr=root</span><br><span class="line"></span><br><span class="line">who=`whoami`</span><br><span class="line">if [[ &quot;$who&quot; != &quot;$usr&quot; ]];then</span><br><span class="line">  echo &quot;请使用 root 用户执行或者 sudo ./sync.master.ca.sh&quot;</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo $who</span><br><span class="line"></span><br><span class="line"># 需要从 m01 拷贝的 ca 文件</span><br><span class="line">caFiles=(</span><br><span class="line">/etc/kubernetes/pki/ca.crt</span><br><span class="line">/etc/kubernetes/pki/ca.key</span><br><span class="line">/etc/kubernetes/pki/sa.key</span><br><span class="line">/etc/kubernetes/pki/sa.pub</span><br><span class="line">/etc/kubernetes/pki/front-proxy-ca.crt</span><br><span class="line">/etc/kubernetes/pki/front-proxy-ca.key</span><br><span class="line">/etc/kubernetes/pki/etcd/ca.crt</span><br><span class="line">/etc/kubernetes/pki/etcd/ca.key</span><br><span class="line">/etc/kubernetes/admin.conf</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">pkiDir=/etc/kubernetes/pki/etcd</span><br><span class="line">for h in $vhost </span><br><span class="line">do</span><br><span class="line"></span><br><span class="line">  ssh $&#123;usr&#125;@$h &quot;mkdir -p $pkiDir&quot;</span><br><span class="line">  </span><br><span class="line">  echo &quot;Dirs for ca scp created, start to scp...&quot;</span><br><span class="line"></span><br><span class="line">  # scp 文件到目标机</span><br><span class="line">  for f in $&#123;caFiles[@]&#125;</span><br><span class="line">  do </span><br><span class="line">    echo &quot;scp $f $&#123;usr&#125;@$h:$f&quot;</span><br><span class="line">    scp $f $&#123;usr&#125;@$h:$f</span><br><span class="line">  done</span><br><span class="line"></span><br><span class="line">  echo &quot;Ca files transfered for $h ... ok&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>执行脚本，将 tf-k8s-m1 相关的 ca 文件传到tf-k8s-m2 和 tf-k8s-m3：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chmod +x sync.master.ca.sh</span><br><span class="line"></span><br><span class="line">sudo ./syncaster.ca.sh</span><br></pre></td></tr></table></figure>
<h4 id="2-5-2-安装master-tf-k8s-m2"><a href="#2-5-2-安装master-tf-k8s-m2" class="headerlink" title="2.5.2 安装master tf-k8s-m2"></a>2.5.2 安装master tf-k8s-m2</h4><p>总共分为四个步骤，分别是:总1. 共分为四个步骤，分别是:</p>
<ul>
<li>配置证书、初始化 kubelet 配置和启动 kubelet</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm init phase certs all --config kubeadm-config.tf-k8s-m2.yaml</span><br><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm init phase etcd local --config kubeadm-config.tf-k8s-m2.yaml</span><br><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm init phase kubeconfig kubelet --config kubeadm-config.tf-k8s-m2.yaml</span><br><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm init phase kubelet-start --config kubeadm-config.tf-k8s-m2.yaml</span><br></pre></td></tr></table></figure>
<ul>
<li>将etcd加入集群</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m2 root]$ kubectl exec -n kube-system etcd-tf-k8s-m1 -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://172.21.0.17:2379 member add tf-k8s-m2 https://172.21.0.42:2380</span><br></pre></td></tr></table></figure>
<p>启动kube-apiserver、kube-controller-manager、kube-scheduler</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm init phase kubeconfig all --config kubeadm-config.m02.yaml</span><br><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm init phase control-plane all --config kubeadm-config.m02.yaml</span><br></pre></td></tr></table></figure>
<p>将节点标记为master节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m2 ~]$ sudo kubeadm init phase mark-control-plane --config kubeadm-config.m02.yaml</span><br></pre></td></tr></table></figure>
<h4 id="2-5-3-安装master-tf-k8s-m3"><a href="#2-5-3-安装master-tf-k8s-m3" class="headerlink" title="2.5.3 安装master tf-k8s-m3"></a>2.5.3 安装master tf-k8s-m3</h4><p>安装过程和安装master tf-k8s-m2是一样的，区别在于使用的kubeadm配置文件为kubeadm-config.tf-k8s-m3.yaml以及etcd加入成员时指定的实例地址不一样。<br>完整的流程如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 1.  配置证书、初始化 kubelet 配置和启动 kubelet</span><br><span class="line">sudo kubeadm init phase certs all --config kubeadm-config.tf-k8s-m3.yaml</span><br><span class="line">sudo kubeadm init phase etcd local --config kubeadm-config.tf-k8s-m3.yaml</span><br><span class="line">sudo kubeadm init phase kubeconfig kubelet --config kubeadm-config.tf-k8s-m3.yaml</span><br><span class="line">sudo kubeadm init phase kubelet-start --config kubeadm-config.tf-k8s-m3.yaml</span><br><span class="line"></span><br><span class="line"># 2. 将 etcd 加入集群</span><br><span class="line">kubectl exec -n kube-system etcd-tf-k8s-m1 -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://172.21.0.17:2379 member add tf-k8s-m3 https://172.21.0.18:2380</span><br><span class="line"></span><br><span class="line"># 3. 启动 kube-apiserver、kube-controller-manager、kube-scheduler</span><br><span class="line">sudo kubeadm init phase kubeconfig all --config kubeadm-config.tf-k8s-m3.yaml</span><br><span class="line">sudo kubeadm init phase control-plane all --config kubeadm-config.tf-k8s-m3.yaml</span><br><span class="line"></span><br><span class="line"># 4. 将节点标记为 master 节点</span><br><span class="line">sudo kubeadm init phase mark-control-plane --config kubeadm-config.tf-k8s-m3.yaml</span><br></pre></td></tr></table></figure>
<h4 id="2-5-4-验证三个master节点"><a href="#2-5-4-验证三个master节点" class="headerlink" title="2.5.4 验证三个master节点"></a>2.5.4 验证三个master节点</h4><p>至此，三个 master 节点安装完成，通过 kubectl get pods –all-namespaces 查看当前集群所有 Pod。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m2 ~]$ kubectl  get pods --all-namespaces</span><br><span class="line">NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   coredns-78d4cf999f-j8zsr      1/1     Running   0          170m</span><br><span class="line">kube-system   coredns-78d4cf999f-lw5qx      1/1     Running   0          171m</span><br><span class="line">kube-system   etcd-m01                      1/1     Running   8          5h11m</span><br><span class="line">kube-system   etcd-m02                      1/1     Running   12         97m</span><br><span class="line">kube-system   etcd-m03                      1/1     Running   0          91m</span><br><span class="line">kube-system   kube-apiserver-m01            1/1     Running   9          5h11m</span><br><span class="line">kube-system   kube-apiserver-m02            1/1     Running   0          95m</span><br><span class="line">kube-system   kube-apiserver-m03            1/1     Running   0          91m</span><br><span class="line">kube-system   kube-controller-manager-m01   1/1     Running   4          5h11m</span><br><span class="line">kube-system   kube-controller-manager-m02   1/1     Running   0          95m</span><br><span class="line">kube-system   kube-controller-manager-m03   1/1     Running   0          91m</span><br><span class="line">kube-system   kube-flannel-ds-amd64-7b86z   1/1     Running   0          3h31m</span><br><span class="line">kube-system   kube-flannel-ds-amd64-98qks   1/1     Running   0          91m</span><br><span class="line">kube-system   kube-flannel-ds-amd64-ljcdp   1/1     Running   0          97m</span><br><span class="line">kube-system   kube-proxy-krnjq              1/1     Running   0          5h12m</span><br><span class="line">kube-system   kube-proxy-scb25              1/1     Running   0          91m</span><br><span class="line">kube-system   kube-proxy-xp4rj              1/1     Running   0          97m</span><br><span class="line">kube-system   kube-scheduler-m01            1/1     Running   4          5h11m</span><br><span class="line">kube-system   kube-scheduler-m02            1/1     Running   0          95m</span><br><span class="line">kube-system   kube-scheduler-m03            1/1     Running   0          91m</span><br></pre></td></tr></table></figure>
<h4 id="2-5-5-加入工作节点spark"><a href="#2-5-5-加入工作节点spark" class="headerlink" title="2.5.5 加入工作节点spark"></a>2.5.5 加入工作节点spark</h4><p>这步很简单，只需要在工作节点 tf-k8s-n1 和 tf-k8s-n2 上执行加入集群的命令即可。</p>
<p>可以使用上面安装 master tf-k8s-m1 成功后打印的命令 kubeadm join api.tf-k8s.xiangwushuo.com:6443 –token a1t7c1.mzltpc72dc3wzj9y –discovery-token-ca-cert-hash sha256:05f44b111174613055975f012fc11fe09bdcd746bd7b3c8d99060c52619f8738，也可以重新生成 Token。<br>这里演示如何重新生成 Token 和 证书 hash，在 tf-k8s-m1 上执行以下操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 1. 创建 token</span><br><span class="line">[kube@tf-k8s-m1 ~]$ kubeadm token create </span><br><span class="line"></span><br><span class="line"># 控制台打印如：</span><br><span class="line">gz1v4w.sulpuxkqtnyci92f</span><br><span class="line"></span><br><span class="line"># 2.  查看我们创建的 k8s 集群的证书 hash</span><br><span class="line">[kube@tf-k8s-m1 ~]$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &apos;s/^.* //&apos;</span><br><span class="line"></span><br><span class="line"># 控制台打印如：</span><br><span class="line">b125cd0c80462353d8fa3e4f5034f1e1a1e3cc9bade32acfb235daa867c60f61</span><br></pre></td></tr></table></figure>
<p>然后使用<code>kubeadm join</code>,分别在工作节点tf-k8s-n1与tf-k8s-n2上执行，将节点加入<br>集群，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm join api.tf-k8s.xiangwushuo.com:6443 --token gz1v4w.sulpuxkqtnyci92f --discovery-token-ca-cert-hash sha256:b125cd0c80462353d8fa3e4f5034f1e1a1e3cc9bade32acfb235daa867c60f61</span><br></pre></td></tr></table></figure>
<p>在 tf-k8s-m1 上通过 kubectl get nodes 查看，将看到节点已被加进来（节点刚加进来时，状态可能会是 NotReady，稍等一会就回变成 Ready）。</p>
<h3 id="2-6-部署高可用CoreDNS"><a href="#2-6-部署高可用CoreDNS" class="headerlink" title="2.6 部署高可用CoreDNS"></a>2.6 部署高可用CoreDNS</h3><p>默认安装的 CoreDNS 存在单点问题。在 m01 上通过 kubectl get pods -n kube-system -owide 查看当前集群 CoreDNS Pod 分布（如下）。</p>
<p>从列表中，可以看到 CoreDNS 的两个 Pod 都在 m01 上，存在单点问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[kube@tf-k8s-m1 ~]$ kubectl get pods -n kube-system -owide</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-6c67f849c7-h7lcr                1/1     Running   0          4d3h    10.244.3.2    tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-6c67f849c7-mx9k9                1/1     Running   0          4d3h    10.244.4.2    tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-tf-k8s-m1                          1/1     Running   1          4d5h    172.21.0.17   tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-tf-k8s-m2                          1/1     Running   7          4d3h    172.21.0.42   tf-k8s-m2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-tf-k8s-m3                          1/1     Running   7          4d3h    172.21.0.18   tf-k8s-m3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-tf-k8s-m1                1/1     Running   0          4d5h    172.21.0.17   tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-tf-k8s-m2                1/1     Running   0          4d3h    172.21.0.42   tf-k8s-m2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-tf-k8s-m3                1/1     Running   0          4d3h    172.21.0.18   tf-k8s-m3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-tf-k8s-m1       1/1     Running   1          4d5h    172.21.0.17   tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-tf-k8s-m2       1/1     Running   0          4d3h    172.21.0.42   tf-k8s-m2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-tf-k8s-m3       1/1     Running   0          4d3h    172.21.0.18   tf-k8s-m3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-4v6dd             1/1     Running   1          4d3h    172.21.0.47   tf-k8s-n2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-g6sg5             1/1     Running   0          4d3h    172.21.0.18   tf-k8s-m3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-ml4w7             1/1     Running   1          4d3h    172.21.0.20   tf-k8s-n1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-tb27x             1/1     Running   0          4d3h    172.21.0.42   tf-k8s-m2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-flannel-ds-amd64-x5dqj             1/1     Running   0          4d4h    172.21.0.17   tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-4wbn7                        1/1     Running   0          4d3h    172.21.0.20   tf-k8s-n1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-8dhtz                        1/1     Running   0          4d3h    172.21.0.42   tf-k8s-m2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-l8727                        1/1     Running   0          4d5h    172.21.0.17   tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-tz924                        1/1     Running   0          4d3h    172.21.0.47   tf-k8s-n2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-w7tmn                        1/1     Running   0          4d3h    172.21.0.18   tf-k8s-m3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-tf-k8s-m1                1/1     Running   1          4d5h    172.21.0.17   tf-k8s-m1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-tf-k8s-m2                1/1     Running   0          4d3h    172.21.0.42   tf-k8s-m2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-tf-k8s-m3                1/1     Running   0          4d3h    172.21.0.18   tf-k8s-m3   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard-847f8cb7b8-hmf9m   1/1     Running   0          3d23h   10.244.4.4    tf-k8s-n2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-server-8658466f94-pzl6z         1/1     Running   0          4d2h    10.244.3.3    tf-k8s-n1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>首先删除CoreDNS的deploy，然后创建新的CoreDNS-HA.yaml配置文件，如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-dns</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  #集群规模可自行配置</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kube-dns</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 25%</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kube-dns</span><br><span class="line">    spec:</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">          - weight: 100</span><br><span class="line">            podAffinityTerm:</span><br><span class="line">              labelSelector:</span><br><span class="line">                matchExpressions:</span><br><span class="line">                - key: k8s-app</span><br><span class="line">                  operator: In</span><br><span class="line">                  values:</span><br><span class="line">                  - kube-dns</span><br><span class="line">              topologyKey: kubernetes.io/hostname</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - -conf</span><br><span class="line">        - /etc/coredns/Corefile</span><br><span class="line">        image: registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        livenessProbe:</span><br><span class="line">          failureThreshold: 5</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /health</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        name: coredns</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns</span><br><span class="line">          protocol: UDP</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns-tcp</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 9153</span><br><span class="line">          name: metrics</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 170Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 70Mi</span><br><span class="line">        securityContext:</span><br><span class="line">          allowPrivilegeEscalation: false</span><br><span class="line">          capabilities:</span><br><span class="line">            add:</span><br><span class="line">            - NET_BIND_SERVICE</span><br><span class="line">            drop:</span><br><span class="line">            - all</span><br><span class="line">          readOnlyRootFilesystem: true</span><br><span class="line">        terminationMessagePath: /dev/termination-log</span><br><span class="line">        terminationMessagePolicy: File</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /etc/coredns</span><br><span class="line">          name: config-volume</span><br><span class="line">          readOnly: true</span><br><span class="line">      dnsPolicy: Default</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">      securityContext: &#123;&#125;</span><br><span class="line">      serviceAccount: coredns</span><br><span class="line">      serviceAccountName: coredns</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: CriticalAddonsOnly</span><br><span class="line">        operator: Exists</span><br><span class="line">      - effect: NoSchedule</span><br><span class="line">        key: node-role.kubernetes.io/master</span><br><span class="line">      volumes:</span><br><span class="line">      - configMap:</span><br><span class="line">          defaultMode: 420</span><br><span class="line">          items:</span><br><span class="line">          - key: Corefile</span><br><span class="line">            path: Corefile</span><br><span class="line">          name: coredns</span><br><span class="line">        name: config-volume</span><br></pre></td></tr></table></figure>
<p>部署新的CoreDNS </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f CoreDNS-HA.yaml</span><br></pre></td></tr></table></figure>
<h3 id="2-7-部署监控组件metrics-server"><a href="#2-7-部署监控组件metrics-server" class="headerlink" title="2.7 部署监控组件metrics-server"></a>2.7 部署监控组件metrics-server</h3>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/kubernetes/" rel="tag"># kubernetes</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/01/31/基于docker-cookiercutter创建data-science项目/" rel="next" title="基于docker-cookiercutter创建data-science项目">
                <i class="fa fa-chevron-left"></i> 基于docker-cookiercutter创建data-science项目
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chubby Hsia</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、环境准备"><span class="nav-number">1.</span> <span class="nav-text">一、环境准备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-硬件设备环境"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 硬件设备环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-软件环境"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 软件环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-相关系统设置"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 相关系统设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-配置集群内各个机器之间的免密码登录"><span class="nav-number">1.4.</span> <span class="nav-text">1.3 配置集群内各个机器之间的免密码登录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-配置hosts"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.3.1 配置hosts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-新建用户"><span class="nav-number">1.4.2.</span> <span class="nav-text">1.3.2 新建用户</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-3-设置免密登录"><span class="nav-number">1.4.3.</span> <span class="nav-text">1.3.3 设置免密登录</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、安装步骤"><span class="nav-number">2.</span> <span class="nav-text">二、安装步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-安装docker"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 安装docker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-安装kubernetes-yum源和kubelet、kubeadm、kubectl组件"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 安装kubernetes yum源和kubelet、kubeadm、kubectl组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-所有机器上配置-kubernetes-repo-yum-源"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 所有机器上配置 kubernetes.repo yum 源</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-所有机器上安装-kubelet、kubeadm、kubectl组件"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 所有机器上安装 kubelet、kubeadm、kubectl组件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-初始化kubeadm配置文件"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 初始化kubeadm配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-安装master镜像和执行kubeadm初始化"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 安装master镜像和执行kubeadm初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-拉取镜像到本地"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.4.1 拉取镜像到本地</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-2-安装master-tf-k8s-m1"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2 安装master tf-k8s-m1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-3-kube用户配置"><span class="nav-number">2.4.3.</span> <span class="nav-text">2.4.3 kube用户配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-4-安装CNI插件flannel"><span class="nav-number">2.4.4.</span> <span class="nav-text">2.4.4 安装CNI插件flannel</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-安装剩余的master"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 安装剩余的master</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-1-同步tf-k8s-m1的ca证书"><span class="nav-number">2.5.1.</span> <span class="nav-text">2.5.1 同步tf-k8s-m1的ca证书</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-2-安装master-tf-k8s-m2"><span class="nav-number">2.5.2.</span> <span class="nav-text">2.5.2 安装master tf-k8s-m2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-3-安装master-tf-k8s-m3"><span class="nav-number">2.5.3.</span> <span class="nav-text">2.5.3 安装master tf-k8s-m3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-4-验证三个master节点"><span class="nav-number">2.5.4.</span> <span class="nav-text">2.5.4 验证三个master节点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-5-加入工作节点spark"><span class="nav-number">2.5.5.</span> <span class="nav-text">2.5.5 加入工作节点spark</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-部署高可用CoreDNS"><span class="nav-number">2.6.</span> <span class="nav-text">2.6 部署高可用CoreDNS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-部署监控组件metrics-server"><span class="nav-number">2.7.</span> <span class="nav-text">2.7 部署监控组件metrics-server</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chubby Hsia</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
